{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports & Initializations","metadata":{}},{"cell_type":"code","source":"from __future__ import division\nimport itertools, random, torch, os, time, pickle, psutil, gc, warnings\nfrom sklearn.preprocessing import MinMaxScaler\nfrom torch.autograd import Variable\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport scipy.sparse as sp\nimport torch.nn as nn\nimport pandas as pd\nimport numpy as np\n\nwarnings.filterwarnings(\"ignore\")\n\npath     = \"/kaggle/input/iasdm2drl2022/\"\npath_out = \"/kaggle/working/\"\n\nfactors  = pd.read_csv(path+'factors_returns.csv')\nstrategy = pd.read_csv(path+'strategy_returns.csv')\ndevice   = torch.device(\"cuda\")","metadata":{"execution":{"iopub.status.busy":"2022-06-06T16:18:20.878607Z","iopub.execute_input":"2022-06-06T16:18:20.880102Z","iopub.status.idle":"2022-06-06T16:18:22.067355Z","shell.execute_reply.started":"2022-06-06T16:18:20.880035Z","shell.execute_reply":"2022-06-06T16:18:22.066454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LSTM Model ","metadata":{}},{"cell_type":"code","source":"class LSTM(nn.Module):\n\n    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n        super(LSTM, self).__init__()\n\n        self.num_classes = num_classes\n        self.num_layers = num_layers\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.seq_length = seq_length\n\n        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n                            num_layers=num_layers, batch_first=True)\n\n        self.fc = nn.Linear(hidden_size, num_classes)\n        self.hidden_state = None\n\n    def forward(self, x):\n        h_0 = Variable(torch.zeros(\n            self.num_layers, x.size(0), self.hidden_size)).to(device)\n\n        c_0 = Variable(torch.zeros(\n            self.num_layers, x.size(0), self.hidden_size)).to(device)\n\n        # Propagate input through LSTM\n        ula, (h_out, _) = self.lstm(x, (h_0, c_0))\n        h_out = h_out.view(-1, self.hidden_size)\n        self.hidden_state = h_out\n        out = self.fc(h_out)\n\n        return out","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Time Series Features","metadata":{}},{"cell_type":"code","source":"# Convert df to numpy\ndef convert_df_to_np(df):\n  vals = []\n  for line in df.iterrows():\n    vals.append((line[1].to_numpy()[:]))\n  return np.nan_to_num(vals)\n\n# Convert raw factors data to numpy array of shape (N,11)\ndef raw_to_np(df, val_init):\n  vals = [val_init * np.ones(11)]\n  for line in df.iterrows():\n    vals.append((line[1].to_numpy()[1::]))\n  return vals[1::]\n\n# Normalize the factors\nfactors_processed = np.array(raw_to_np(factors,1))\nfactors_processed -= factors_processed.min()\nfactors_processed /= factors_processed.max()\nfactors_processed = factors_processed * 2 -1\n\nfactors_processed_df = pd.DataFrame(factors_processed)\n\n# Compute factors features\n\nfactors_features = {}\ndef compute_features(factors_features, window_size):\n  factors_features[window_size] = {}\n  ### ROLLING STD\n  vals   = factors_processed_df.rolling(window_size).std()\n  factors_features[window_size][\"rolling std\"] = torch.from_numpy(convert_df_to_np(vals))\n\n  ### ROLLING MEAN\n  vals   = factors_processed_df.rolling(window_size).mean()\n  factors_features[window_size][\"rolling mean\"] = torch.from_numpy(convert_df_to_np(vals))\n\n  ### ROLLING MIN\n  vals   = factors_processed_df.rolling(window_size).min()\n  factors_features[window_size][\"rolling min\"] = torch.from_numpy(convert_df_to_np(vals))\n\n  ### ROLLING MAX\n  vals   = factors_processed_df.rolling(window_size).max()\n  factors_features[window_size][\"rolling max\"] = torch.from_numpy(convert_df_to_np(vals))\n\ncompute_features(factors_features, 5)\ncompute_features(factors_features, 10)\ncompute_features(factors_features, 50)\ncompute_features(factors_features, 100)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# State Design","metadata":{}},{"cell_type":"code","source":"# Create state given date and previous actions\ndef create_states(acs, date):\n\n  #### Take all action_buffer_size previous actions\n  acs = torch.from_numpy(np.array(acs))\n  prev_acs = torch.empty(0)\n  for i in range(1,action_buffer_size+1):\n    if len(acs)>=i: prev_acs = torch.cat((prev_acs, acs[-i]), axis=0)\n    else:           prev_acs = torch.cat((prev_acs, torch.ones(11)), axis=0)\n\n  #### TIME SERIES ON ROLLING STD (feat_buffer_size * 11)\n  feats = torch.empty(0)\n  data  = factors_features[10][\"rolling std\"][:i]\n  for i in range(1,feat_buffer_size+1):\n    if len(data)>=i:\n      feats = torch.cat((feats, data[-i]), axis=0)\n    else:\n      feats = torch.cat((feats, torch.zeros(11)), axis=0)\n\n  #### GENERATE STATE\n  state = torch.cat((feats, factors_features[10][\"rolling mean\"][i], factors_features[10][\"rolling std\"][i], factors_features[50][\"rolling mean\"][i], factors_features[50][\"rolling std\"][i],\n                     torch.from_numpy(factors_processed[i].astype(np.float32)), prev_acs), axis=0).detach()\n\n  return state","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DDPG Implementation\n\nWe used the following PyTorch implementation of DDPG: https://github.com/blackredscarf/pytorch-DDPG","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport shutil\nimport torch.autograd as Variable\n\ndef soft_update(target, source, tau):\n\t\"\"\"\n\tCopies the parameters from source network (x) to target network (y) using the below update\n\ty = TAU*x + (1 - TAU)*y\n\t:param target: Target network (PyTorch)\n\t:param source: Source network (PyTorch)\n\t:return:\n\t\"\"\"\n\tfor target_param, param in zip(target.parameters(), source.parameters()):\n\t\ttarget_param.data.copy_(\n\t\t\ttarget_param.data * (1.0 - tau) + param.data * tau\n\t\t)\n\n\ndef hard_update(target, source):\n\t\"\"\"\n\tCopies the parameters from source network to target network\n\t:param target: Target network (PyTorch)\n\t:param source: Source network (PyTorch)\n\t:return:\n\t\"\"\"\n\tfor target_param, param in zip(target.parameters(), source.parameters()):\n\t\t\ttarget_param.data.copy_(param.data)\n\n\ndef save_training_checkpoint(state, is_best, episode_count):\n\t\"\"\"\n\tSaves the models, with all training parameters intact\n\t:param state:\n\t:param is_best:\n\t:param filename:\n\t:return:\n\t\"\"\"\n\tfilename = str(episode_count) + 'checkpoint.path.rar'\n\ttorch.save(state, filename)\n\tif is_best:\n\t\tshutil.copyfile(filename, 'model_best.pth.tar')\n\n\n# Based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\nclass OrnsteinUhlenbeckActionNoise:\n\n\tdef __init__(self, action_dim, mu = 0, theta = 0.15, sigma = 0.2):\n\t\tself.action_dim = action_dim\n\t\tself.mu = mu\n\t\tself.theta = theta\n\t\tself.sigma = sigma\n\t\tself.X = np.ones(self.action_dim) * self.mu\n\n\tdef reset(self):\n\t\tself.X = np.ones(self.action_dim) * self.mu\n\n\tdef sample(self):\n\t\tdx = self.theta * (self.mu - self.X)\n\t\tdx = dx + self.sigma * np.random.randn(len(self.X))\n\t\tself.X = self.X + dx\n\t\treturn self.X\n\n\n# use this to plot Ornstein Uhlenbeck random motion\nif __name__ == '__main__':\n\tou = OrnsteinUhlenbeckActionNoise(1)\n\tstates = []\n\tfor i in range(1000):\n\t\tstates.append(ou.sample())\n\timport matplotlib.pyplot as plt\n\n\nimport numpy as np\nimport random\nfrom collections import deque\n\n\nclass MemoryBuffer:\n\n\tdef __init__(self, size):\n\t\tself.buffer = deque(maxlen=size)\n\t\tself.maxSize = size\n\t\tself.len = 0\n\n\tdef sample(self, count):\n\t\t\"\"\"\n\t\tsamples a random batch from the replay memory buffer\n\t\t:param count: batch size\n\t\t:return: batch (numpy array)\n\t\t\"\"\"\n\t\tbatch = []\n\t\tcount = min(count, self.len)\n\t\tbatch = random.sample(self.buffer, count)\n\n\t\ts_arr = np.float32([arr[0] for arr in batch])\n\t\ta_arr = np.float32([arr[1] for arr in batch])\n\t\tr_arr = np.float32([arr[2] for arr in batch])\n\t\ts1_arr = np.float32([arr[3] for arr in batch])\n\n\t\treturn s_arr, a_arr, r_arr, s1_arr\n\n\tdef len(self):\n\t\treturn self.len\n\n\tdef add(self, s, a, r, s1):\n\t\t\"\"\"\n\t\tadds a particular transaction in the memory buffer\n\t\t:param s: current state\n\t\t:param a: action taken\n\t\t:param r: reward received\n\t\t:param s1: next state\n\t\t:return:\n\t\t\"\"\"\n\t\ttransition = (s,a,r,s1)\n\t\tself.len += 1\n\t\tif self.len > self.maxSize:\n\t\t\tself.len = self.maxSize\n\t\tself.buffer.append(transition)\n\n\tdef save(self):\n\t\ttorch.save({\"dict\":self.buffer},path+\"buffer.pth\")\n\n\tdef load(self):\n\t\tself.buffer = torch.load(path+\"buffer.pth\")[\"dict\"]\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\nEPS = 0.003\n\ndef fanin_init(size, fanin=None):\n\tfanin = fanin or size[0]\n\tv = 1. / np.sqrt(fanin)\n\treturn torch.Tensor(size).uniform_(-v, v)\n\nclass Critic(nn.Module):\n\n\tdef __init__(self, state_dim, action_dim):\n\t\t\"\"\"\n\t\t:param state_dim: Dimension of input state (int)\n\t\t:param action_dim: Dimension of input action (int)\n\t\t:return:\n\t\t\"\"\"\n\t\tsuper(Critic, self).__init__()\n\n\t\tself.lstm = LSTM(11, 11, 128, 1).to(device)\n\n\t\tself.state_dim = state_dim\n\t\tself.action_dim = action_dim\n\n\t\tself.fcs1 = nn.Linear(state_dim,256)\n\t\tself.fcs1.weight.data = fanin_init(self.fcs1.weight.data.size())\n\t\tself.fcs2 = nn.Linear(256,128)\n\t\tself.fcs2.weight.data = fanin_init(self.fcs2.weight.data.size())\n\n\t\tself.fca1 = nn.Linear(action_dim,128)\n\t\tself.fca1.weight.data = fanin_init(self.fca1.weight.data.size())\n\n\t\tself.fc2 = nn.Linear(256,128)\n\t\tself.fc2.weight.data = fanin_init(self.fc2.weight.data.size())\n\n\t\tself.fc3 = nn.Linear(128,1)\n\t\tself.fc3.weight.data.uniform_(-EPS,EPS)\n\n\tdef forward(self, state, action):\n\t\t\"\"\"\n\t\treturns Value function Q(s,a) obtained from critic network\n\t\t:param state: Input state (Torch Variable : [n,state_dim] )\n\t\t:param action: Input Action (Torch Variable : [n,action_dim] )\n\t\t:return: Value function : Q(S,a) (Torch Variable : [n,1] )\n\t\t\"\"\"\n\n\n\n\t\tstate_1 = state[:,0:(feat_buffer_size*11)]\n\n\n\n\t\tstate_2 = state[:,(feat_buffer_size*11):]\n\n\n\n\t\tout1    = self.lstm(state_1.reshape((-1,feat_buffer_size,11)))\n\t\tstate   = torch.cat((out1,state_2),dim=1)\n\n\t\ts1 = F.relu(self.fcs1(state))\n\t\ts2 = F.relu(self.fcs2(s1))\n\t\ta1 = F.relu(self.fca1(action))\n\t\tx = torch.cat((s2,a1),dim=1)\n\n\t\tx = F.relu(self.fc2(x))\n\t\tx = self.fc3(x)\n\n\t\treturn x\n\n\nclass Actor(nn.Module):\n\n\tdef __init__(self, state_dim, action_dim, action_lim):\n\t\t\"\"\"\n\t\t:param state_dim: Dimension of input state (int)\n\t\t:param action_dim: Dimension of output action (int)\n\t\t:param action_lim: Used to limit action in [-action_lim,action_lim]\n\t\t:return:\n\t\t\"\"\"\n\t\tsuper(Actor, self).__init__()\n\n\t\tself.lstm = LSTM(11, 11, 128, 1).to(device)\n\n\n\t\tself.state_dim = state_dim\n\t\tself.action_dim = action_dim\n\t\tself.action_lim = action_lim.to(device)\n\n\t\tself.fc1 = nn.Linear(state_dim,256)\n\t\tself.fc1.weight.data = fanin_init(self.fc1.weight.data.size())\n\n\t\tself.fc2 = nn.Linear(256,128)\n\t\tself.fc2.weight.data = fanin_init(self.fc2.weight.data.size())\n\n\t\tself.fc3 = nn.Linear(128,64)\n\t\tself.fc3.weight.data = fanin_init(self.fc3.weight.data.size())\n\n\t\tself.fc4 = nn.Linear(64,action_dim)\n\t\tself.fc4.weight.data.uniform_(-EPS,EPS)\n\n\tdef forward(self, state):\n\t\t\"\"\"\n\t\treturns policy function Pi(s) obtained from actor network\n\t\tthis function is a gaussian prob distribution for all actions\n\t\twith mean lying in (-1,1) and sigma lying in (0,1)\n\t\tThe sampled action can , then later be rescaled\n\t\t:param state: Input state (Torch Variable : [n,state_dim] )\n\t\t:return: Output action (Torch Variable: [n,action_dim] )\n\t\t\"\"\"\n\n\t\tstate_1 = state[:,0:(feat_buffer_size*11)].to(device)\n\t\tstate_2 = state[:,(feat_buffer_size*11):].to(device)\n\t\tout1    = self.lstm(state_1.reshape((-1,feat_buffer_size,11)))\n\n\t\tstate   = torch.cat((out1,state_2),dim=1)\n\n\t\tx = F.relu(self.fc1(state))\n\t\tx = F.relu(self.fc2(x))\n\t\tx = F.relu(self.fc3(x))\n\t\taction = F.tanh(self.fc4(x))\n\n\t\taction = action * self.action_lim\n\n\t\treturn action\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport numpy as np\nimport math\n\nBATCH_SIZE = 512\nLEARNING_RATE = 0.00001\nGAMMA = 0.99\nTAU = 0.001\n\nclass Trainer:\n\n  def __init__(self, state_dim, action_dim, action_lim, ram):\n    \"\"\"\n    :param state_dim: Dimensions of state (int)\n    :param action_dim: Dimension of action (int)\n    :param action_lim: Used to limit action in [-action_lim,action_lim]\n    :param ram: replay memory buffer object\n    :return:\n    \"\"\"\n    self.state_dim = state_dim\n    self.action_dim = action_dim\n    self.action_lim = action_lim\n    self.ram = ram\n    self.iter = 0\n    self.noise = OrnsteinUhlenbeckActionNoise(self.action_dim)\n\n    self.actor = Actor(self.state_dim, self.action_dim, self.action_lim).to(device)\n    self.target_actor = Actor(self.state_dim, self.action_dim, self.action_lim).to(device)\n    self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),LEARNING_RATE)\n\n    self.critic = Critic(self.state_dim, self.action_dim).to(device)\n    self.target_critic = Critic(self.state_dim, self.action_dim).to(device)\n    self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),LEARNING_RATE)\n\n    hard_update(self.target_actor, self.actor)\n    hard_update(self.target_critic, self.critic)\n\n  def get_exploitation_action(self, state):\n    \"\"\"\n    gets the action from target actor added with exploration noise\n    :param state: state (Numpy array)\n    :return: sampled action (Numpy array)\n    \"\"\"\n    state = Variable(torch.from_numpy(state))\n    action = self.target_actor.forward(state.to(device)).detach()\n    return action.data.cpu().numpy()\n\n  def get_exploration_action(self, state):\n    global nb_ep\n    \"\"\"\n    gets the action from actor added with exploration noise\n    :param state: state (Numpy array)\n    :return: sampled action (Numpy array)\n    \"\"\"\n    state = Variable(torch.from_numpy(state))\n    action = self.actor.forward(state.to(device)).detach()\n\n    factor   = random.randint(0,10)\n    n_action = action.data.cpu().detach().numpy()\n    n_action[0][factor] += (random.random()*2-1) * 0.01\n    new_action = np.clip(n_action,-1,1)\n    return new_action\n\n  def optimize(self):\n    \"\"\"\n    Samples a random batch from replay memory and performs optimization\n    :return:\n    \"\"\"\n    s1,a1,r1,s2 = self.ram.sample(BATCH_SIZE)\n\n    s1 = Variable(torch.from_numpy(s1).to(device))\n    a1 = Variable(torch.from_numpy(a1).to(device))\n    r1 = Variable(torch.from_numpy(r1).to(device))\n    s2 = Variable(torch.from_numpy(s2).to(device))\n\n    # ---------------------- optimize critic ----------------------\n    # Use target actor exploitation policy here for loss evaluation\n    a2 = self.target_actor.forward(s2).detach()\n\n    next_val = torch.squeeze(self.target_critic.forward(s2, a2).detach())\n\n    # y_exp = r + gamma*Q'( s2, pi'(s2))\n    y_expected = r1 + GAMMA*next_val\n    # y_pred = Q( s1, a1)\n    y_predicted = torch.squeeze(self.critic.forward(s1, a1))\n\n    # compute critic loss, and update the critic\n    loss_critic = F.smooth_l1_loss(y_predicted, y_expected)\n\n    self.critic_optimizer.zero_grad()\n    loss_critic.backward()\n    self.critic_optimizer.step()\n\n    # ---------------------- optimize actor ----------------------\n    pred_a1 = self.actor.forward(s1)\n    loss_actor = -1*torch.sum(self.critic.forward(s1, pred_a1))\n    self.actor_optimizer.zero_grad()\n    loss_actor.backward()\n    self.actor_optimizer.step()\n\n    soft_update(self.target_actor, self.actor, TAU)\n    soft_update(self.target_critic, self.critic, TAU)\n\n    # if self.iter % 100 == 0:\n    # \tprint 'Iteration :- ', self.iter, ' Loss_actor :- ', loss_actor.data.numpy(),\\\n    # \t\t' Loss_critic :- ', loss_critic.data.numpy()\n    # self.iter += 1\n\n  def save_models(self, episode_count):\n    \"\"\"\n    saves the target actor and critic models\n    :param episode_count: the count of episodes iterated\n    :return:\n    \"\"\"\n    torch.save(self.target_actor.state_dict(), path_out + str(episode_count) + '_actor.pt')\n    torch.save(self.target_critic.state_dict(), path_out + str(episode_count) + '_critic.pt')\n    print('Models saved successfully')\n\n  def load_models(self, episode):\n    \"\"\"\n    loads the target actor and critic models, and copies them onto actor and critic models\n    :param episode: the count of episodes iterated (used to find the file name)\n    :return:\n    \"\"\"\n    self.actor.load_state_dict(torch.load(path_out + str(episode) + '_actor.pt'))\n    self.critic.load_state_dict(torch.load(path_out + str(episode) + '_critic.pt'))\n    hard_update(self.target_actor, self.actor)\n    hard_update(self.target_critic, self.critic)\n    print('Models loaded succesfully')\n\ndef reward(weights, factors_returns, strategy_returns):\n    pred_returns = (1 + (weights * factors_returns).sum(axis=1)).cumprod(\n        ).pct_change().fillna(0)\n    tracking_error =  (pred_returns.values - strategy_returns.iloc[:,0].values\n        ) * np.sqrt(250) * np.sqrt(weights.shape[1]+1)\n    turn_over = 0.0020 * 365 * ((weights - weights.shift(1)).abs().fillna(0).values\n        ) / ((weights.index[-1] -weights.index[0]).days) * np.sqrt(\n        weights.shape[0] * (weights.shape[1]+1))\n    error_terms = np.concatenate([tracking_error, turn_over.flatten()], axis=0)\n\n    return -np.sqrt(np.mean(error_terms**2))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"\n# Import factors & strategy returns\ndateparse = lambda x: datetime.strptime(x, '%Y-%m-%d')\nfactors_returns  = pd.read_csv(path+'factors_returns.csv', index_col=0,\n                      parse_dates=True, date_parser=dateparse)\nstrategy_returns = pd.read_csv(path+'strategy_returns.csv', index_col=0,\n                      parse_dates=True, date_parser=dateparse)\n\n# Hyperparameters\nuse_gpu = True\naction_buffer_size = 1\nfeat_buffer_size   = 25\n\nMAX_EPISODES = 1000\nMAX_STEPS    = 3058\nMAX_BUFFER   = 100000\n\nS_DIM = 11 + (11*4) + 11 + (11*1) # LSTM OUTPUT + FACTORS FEATURES + CURRENT FACTORS + PREVIOUS ACTIONS\nA_DIM = 11\nA_MAX = torch.ones(11)\n\nram     = MemoryBuffer(MAX_BUFFER)\ntrainer = Trainer(S_DIM, A_DIM, A_MAX, ram)\n\nprint(' State Dimensions :- ', S_DIM)\nprint(' Action Dimensions :- ', A_DIM)\nprint(' Action Max :- ', A_MAX)\n\nram     = MemoryBuffer(MAX_BUFFER)\ntrainer = Trainer(S_DIM, A_DIM, A_MAX, ram)\n\ndate = 0\nacs  = []\ntraining_rews = []\n\ntrainer.load_models(110)\n\nfor _ep in range(111,MAX_EPISODES):\n\n  print('EPISODE :- ', _ep)\n  date = 0\n  acs  = []\n  observation = create_states(acs, 0).detach()\n  weights     = pd.DataFrame(index=[], columns =[], data=[])\n\n  for r in range(MAX_STEPS):\n    state = np.float32(observation)\n\n    # Compute action (exploration or exploitation)\n    if _ep%10 == 0: action = trainer.get_exploitation_action(state.reshape(1,-1))[0]\n    else:           action = trainer.get_exploration_action(state.reshape(1,-1))[0]\n    acs.append(action)\n\n    # Update obs, next obs, done & date\n    date += 1\n    new_observation = create_states(acs, date).detach().numpy()\n    observation     = new_observation\n    done            = (date == MAX_STEPS)\n\n    # Update weights\n    new_row = pd.DataFrame(index=factors_returns[date-1:date].index, columns = factors_returns[date-1:date].columns, data=acs[date-1:date])\n    weights = weights.append(new_row)\n\n    # Compute immediate reward\n    if date > 2:\n      rew = reward(weights, factors_returns[:(date)], strategy_returns[:(date)])\n    else:\n      rew = 0\n\n    if date % 100 == 0:\n        print(rew)\n\n    ############################\n    if date == 3058:\n        if _ep%10 != 0:\n            print(f\"Train - \",rew)\n        else:\n            print(\"#############\")\n            print(f\"EVAL - \",rew)\n            print(\"#############\")\n    ############################\n\n    # Compute next state & update replay buffer (ram)\n    new_state = np.float32(new_observation)\n    ram.add(state, action, rew, new_state)\n\n    if done:\n      if _ep%10 == 0:\n        training_rews.append(rew)\n      break\n\n  # check memory consumption and clear memory\n  gc.collect()\n\n  # Optimize when we have seen at least 1 episode\n  if _ep > 1:\n    for i in range(100):\n      trainer.optimize()\n\n  # Save model every 10 episodes\n  if _ep != 0 and _ep%10 == 0:\n    trainer.save_models(_ep)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"MODEL_TO_SUBMIT = 160","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(S_DIM, A_DIM, A_MAX, ram)\ntrainer.load_models(MODEL_TO_SUBMIT)\n\ndate          = 0\nacs           = []\ntraining_rews = []\nobservation   = create_states(acs, 0).detach()\nfor r in range(3058):\n  \n  state  = np.float32(observation)\n  action = trainer.get_exploitation_action(state.reshape(1,-1))[0]\n  acs.append(action)\n\n  date += 1\n\n  if date != 3058:\n    new_observation = create_states(acs, date).detach().numpy()\n    observation     = new_observation\n  if date == 3058:\n    weights = pd.DataFrame(index=factors_returns[:(date)].index, columns = factors_returns[:(date)].columns, data=acs[-(date):])\n    rew = reward(weights, factors_returns[:(date)], strategy_returns[:(date)])\n    print(rew)\n\ndf = create_submission(weights,factors_returns)\ndf.to_csv(path_out+'final_submission.csv')","metadata":{},"execution_count":null,"outputs":[]}]}